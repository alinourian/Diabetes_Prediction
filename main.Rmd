---
title: "R Notebook"
output: html_notebook
---

## ======================= Import Libraries & Load Data ========================

```{r}
library(data.table)
library(ggplot2)
library(glmnet)
library(caret)
library(randomForest)
library(class)
library(e1071)
library(MASS)

dataset = fread('diabetes_binary_5050split_health_indicators_BRFSS2015.csv')

dataset$Diabetes_binary = as.factor(dataset$Diabetes_binary)
head(dataset, 5)
```
## ================================ Question 1 =================================

```{r}
ggplot(dataset, aes(BMI, fill = Diabetes_binary))+
  geom_density(alpha = .75)
```

```{r}
dataset$one = 1
ds = dataset[, .(n = sum(one)), .(Diabetes_binary, NoDocbcCost)]
ds[, n_total := sum(n), .(NoDocbcCost)]
ds[, n_percent := n / n_total]

ggplot(ds, aes(as.factor(NoDocbcCost), n_percent, fill = Diabetes_binary))+
  geom_bar(stat = 'identity', )
```


```{r}
summary(dataset)
```

```{r}
pairs(dataset[, c("GenHlth", "MentHlth", "PhysHlth", "Income", "BMI", "Age", "Education", "Diabetes_binary")])
```

```{r}
# Histogram of HighBP
ggplot(dataset, aes(x = HighBP)) +
  geom_histogram(fill = "blue", binwidth = 0.5, alpha = 0.8)

# Histogram of HighChol
ggplot(dataset, aes(x = HighChol)) +
  geom_histogram(fill = "orange", binwidth = 0.5, alpha = 0.8)

# Histogram of CholCheck
ggplot(dataset, aes(x = CholCheck)) +
  geom_histogram(fill = "green", binwidth = 0.5, alpha = 0.8)

# Histogram of BMI
ggplot(dataset, aes(x = BMI)) +
  geom_histogram(fill = "red", binwidth = 1, alpha = 0.8)

# Histogram of CholCheck
ggplot(dataset, aes(x = Smoker)) +
  geom_histogram(fill = "yellow", binwidth = 0.5, alpha = 0.8)

# Histogram of PhysActivity
ggplot(dataset, aes(x = PhysActivity)) +
  geom_histogram(fill = "purple", binwidth = 0.5, alpha = 0.8)

# Histogram of HvyAlcoholConsump
ggplot(dataset, aes(x = HvyAlcoholConsump)) +
  geom_histogram(fill = "pink", binwidth = 0.5, alpha = 0.8)

# Histogram of GenHlth
ggplot(dataset, aes(x = GenHlth)) +
  geom_histogram(fill = "blue", binwidth = 0.5, alpha = 0.8)

# Histogram of MentHlth
ggplot(dataset, aes(x = MentHlth)) +
  geom_histogram(fill = "orange", binwidth = 0.8, alpha = 0.8)

# Histogram of PhysHlth
ggplot(dataset, aes(x = PhysHlth)) +
  geom_histogram(fill = "green", binwidth = 0.8, alpha = 0.8)

# Histogram of Sex
ggplot(dataset, aes(x = Sex)) +
  geom_histogram(fill = "red", binwidth = 0.5, alpha = 0.8)

# Histogram of Age
ggplot(dataset, aes(x = Age)) +
  geom_histogram(fill = "yellow", binwidth = 0.5, alpha = 0.8)

# Histogram of Education
ggplot(dataset, aes(x = Education)) +
  geom_histogram(fill = "purple", binwidth = 0.5, alpha = 0.8)

# Histogram of Income
ggplot(dataset, aes(x = Income)) +
  geom_histogram(fill = "pink", binwidth = 0.5, alpha = 0.8)
```
```{r}
color = c("blue", "red")

mosaicplot(HighBP ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(HighChol ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(CholCheck ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(Stroke ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(HeartDiseaseorAttack ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(Sex ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(Smoker ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(PhysActivity ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(Veggies ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(Fruits ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(HvyAlcoholConsump ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(AnyHealthcare ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(NoDocbcCost ~ Diabetes_binary, data = dataset, color=color)
mosaicplot(DiffWalk ~ Diabetes_binary, data = dataset, color=color)
```
```{r}
# Boxplot of BMI by diabetes status
ggplot(dataset, aes(x = BMI, y = Diabetes_binary, color = Diabetes_binary, group = Diabetes_binary)) +
  geom_boxplot(alpha = 0.75)

# Boxplot of GenHlth by diabetes status
ggplot(dataset, aes(x = GenHlth, y = Diabetes_binary, color = Diabetes_binary, group = Diabetes_binary)) +
  geom_boxplot(alpha = 0.75)

# Boxplot of MentHlth by diabetes status
ggplot(dataset, aes(x = MentHlth, y = Diabetes_binary, color = Diabetes_binary, group = Diabetes_binary)) +
  geom_boxplot(alpha = 0.75)

# Boxplot of PhysHlth by diabetes status
ggplot(dataset, aes(x = PhysHlth, y = Diabetes_binary, color = Diabetes_binary, group = Diabetes_binary)) +
  geom_boxplot(alpha = 0.75)

# Boxplot of Age by diabetes status
ggplot(dataset, aes(x = Age, y = Diabetes_binary, color = Diabetes_binary, group = Diabetes_binary)) +
  geom_boxplot(alpha = 0.75)

# Boxplot of Education by diabetes status
ggplot(dataset, aes(x = Education, y = Diabetes_binary, color = Diabetes_binary, group = Diabetes_binary)) +
  geom_boxplot(alpha = 0.75)

# Boxplot of Income by diabetes status
ggplot(dataset, aes(x = Income, y = Diabetes_binary, color = Diabetes_binary, group = Diabetes_binary)) +
  geom_boxplot(alpha = 0.75)
```

Histogram:
A histogram is a graphical representation of the distribution of a continuous or discrete variable. It consists of a series of adjacent rectangular bars, where the width of each bar represents a range of values, and the height represents the frequency or count of observations falling within that range. Histograms provide insights into the shape, center, and spread of the data distribution. They are particularly useful for visualizing the density or frequency of values within different intervals or bins. Histograms can help identify patterns such as skewness, multimodality, and outliers in the data.

As we can see in the histograms, the distribution of some features such as MentHlth, PhysHlth, HvyAlcoholConsump, CholCheck, and HeartDiseaseorAttack are unbalanced, otherwise some of them such as HighChol, Sex and Smoker are almost balanced.

MasiacPlot:
A mosaic plot is a graphical representation that displays the relationship between two or more categorical variables. It consists of rectangular tiles, where the area of each tile is proportional to the count or percentage of observations falling into a specific combination of categories. Mosaic plots can be useful for exploring the associations and dependencies between categorical variables. They provide a visual representation of how the frequencies or proportions of different categories are distributed across the combinations of variables, allowing for easy identification of patterns and relationships.

Based on these graphs, Some features like HighBP, HighChol, CholCheck, Stroke, HeartDiseaseorAttack, PhysActivit, HvyAlcoholConsump and DiffWalk have clear effect on diabetes.

Barplot:
A boxplot, also known as a box-and-whisker plot, is a graphical representation of the distribution of a numerical variable. It displays key statistical measures such as the median, quartiles, and potential outliers. The box in the plot represents the interquartile range (IQR), which contains the middle 50% of the data. The line within the box represents the median. The whiskers extend to the minimum and maximum values within a certain range (usually 1.5 times the IQR), and outliers are depicted as individual data points beyond the whiskers. Boxplots are useful for identifying the central tendency, spread, and skewness of a dataset.

Based on these charts, the effect of Age, BMI, Income, PhysHlth and GenHlth is clear on diabetes.


In conclusoin, as some features are somehow related to diabetes, we can hope there will be a good model to estimate the diabetes based on these data and features.

##  ============================== Question 2 & 3 & 4 =============================

To determine which features in the dataset have the most predictive power for diabetes, we can use feature selection techniques. One common method for feature selection is to calculate the feature importance or coefficient values from a logistic regression model. The absolute values of the coefficients indicate the strength of association between each feature and the target variable.

We split the data into a 70% training set and a 30% test set. The validation measurements we utilize include accuracy, AIC, RSS, or MSE. The models are logistic regression, random forest, KNN, LDA

```{r}
# Load dataset
data <- read.csv("diabetes_binary_5050split_health_indicators_BRFSS2015.csv")

# Shuffle the dataset
set.seed(123)
shuffled_data <- data[sample(nrow(data)), ]

# Split the shuffled data into training and testing sets
train_ratio <- 0.7  # Set the desired training set ratio
train_indices <- createDataPartition(shuffled_data$Diabetes_binary, p = train_ratio, list = FALSE)
train_data <- shuffled_data[train_indices, ]
test_data <- shuffled_data[-train_indices, ]
```


```{r}
# Create a logistic regression model
logistic_model <- glm(Diabetes_binary ~ ., data = train_data, family = binomial)
logistic_predictions <- predict(logistic_model, newdata = test_data, type = "response")
summary_data <- summary(logistic_model)

# Extract p-values for each feature
p_values <- summary_data$coefficients[, "Pr(>|z|)"]

# Extract feature coefficients and their names
coefficients <- coef(logistic_model)[-1]  # Exclude the intercept
feature_names <- names(coefficients)

# Sort the coefficients by absolute value in descending order
sorted_indices <- order(abs(coefficients), decreasing = TRUE)
sorted_coefficients <- coefficients[sorted_indices]
sorted_feature_names <- feature_names[sorted_indices]

# Print the feature names and their corresponding coefficients
for (i in seq_along(sorted_coefficients)) {
  feature <- sorted_feature_names[i]
  coefficient <- sorted_coefficients[i]
  p_value <- p_values[feature]
  cat("Feature:", feature, ",\tCoefficient:", round(coefficient, 5), ",\tp_value:", p_value, "\n")
}
```

it is possible to use a subset of predictors and ignore others. This approach is known as feature selection, where you select a subset of relevant predictors that have the most predictive power for your target variable.

One common method for feature selection is called "stepwise selection," which iteratively adds or removes predictors based on their significance or performance in the model.

Feature selection methods like stepwise selection have their limitations and may not always yield the optimal subset of predictors. It's a good practice to combine feature selection techniques with domain knowledge and consider alternative approaches like regularization methods (e.g., Lasso or Ridge regression) or ensemble methods (e.g., random forests) to select relevant predictors.

Here is the stepwise feature selection:
```{r}
# Create a logistic regression model with stepwise feature selection
model <- step(glm(Diabetes_binary ~ ., data = train_data), direction = "both", trace = TRUE)
summary(model)

# Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "response")
logistic_accuracy <- sum(predictions == test_data$Diabetes_binary) / length(test_data$Diabetes_binary)
cat("Accuracy:", logistic_accuracy, "\n")
```

Using random forest instead of stepwise selection:
```{r}
# Split the data into features and target variable
features <- data[, -1]  # Exclude the target variable
target <- data$Diabetes_binary

# Create a Random Forest model
rf_model <- randomForest(features, target, ntree=100, importance=TRUE, do.trace=10) # ntree=250 comes into a good result
rf_predictions <- predict(rf_model, newdata = test_data, type = "class")
```

```{r}
# Get feature importances
importances <- importance(rf_model)

print(rf_model)
print(importances)

# Plot the variable importance for the model
varImpPlot(rf_model, main = "Variable Importance")

rf_df <- as.data.frame(rf_model$importance)

# Plot the variable importance for the model using all the variables
ggplot(rf_df, aes(x = reorder(rownames(rf_df), IncNodePurity), y = IncNodePurity)) +
  geom_col(fill = "red") +
  coord_flip() +
  labs(x = "Variables", y = "Mean Decrease in Gini", title = "Variable Importance")
```


Using KNN:
```{r}
# Train and evaluate the K-nearest neighbors model
knn_model <- knn(train_data[, -1], test_data[, -1], train_data$Diabetes_binary, k = 5)
knn_predictions <- as.factor(knn_model)
knn_accuracy <- sum(knn_predictions == test_data$Diabetes_binary) / length(test_data$Diabetes_binary)
cat("Accuracy:", knn_accuracy, "\n")
```

As we can see in the results of different models, the error (e.g. MSE or AIC) or variables importance after adding 9 features is significantly reduced. Moreover, the effect of the remaining feature on the error experiences a sudden decline. By Considering all the result, comparing them and also taking a look to the plots presented in the first part, we obtain the final features vector as follow:
```{r}
best_features = c("BMI", "HighBP", "GenHlth", "Age", "PhysHlth", "HighChol", "DiffWalk", "CholCheck", "HvyAlcoholConsump", "Diabetes_binary")

new_data = data[, best_features]
shuffled_data <- new_data[sample(nrow(data)), ]

# Split the shuffled data into training and testing sets
train_ratio <- 0.7  # Set the desired training set ratio
train_indices <- createDataPartition(shuffled_data$Diabetes_binary, p = train_ratio, list = FALSE)
train_data <- shuffled_data[train_indices, ]
test_data <- shuffled_data[-train_indices, ]
```

# logistic
```{r}
# Create a logistic regression model
logistic_model <- glm(Diabetes_binary ~ ., data = train_data, family = binomial)
logistic_predictions <- predict(logistic_model, newdata = test_data, type = "response")
summary_data <- summary(logistic_model)
print(summary_data)
```
# LDA
```{r}
# Extract the predictor variables and the target variable from the train and test data
train_x <- train_data[, -1]
train_y <- train_data[, 1]
test_x <- test_data[, -1]
test_y <- test_data[, 1]

```

```{r}
library(MASS)
library(gmodels)
# Apply Linear Discriminant Analysis (LDA)
lda_model <- lda(train_x, train_y)

# Predict using LDA
lda_pred <- predict(lda_model, test_x)

# Calculate accuracy for LDA
lda_accuracy = sum(lda_pred$class == test_y) / length(test_y)

# Print the accuracies
cat("LDA Accuracy:", lda_accuracy, "\n")
summary(lda_model)
```
not acceptabe!

# KNN
```{r}
# Train and evaluate the K-nearest neighbors model
knn_model <- knn(train_data[, -1], test_data[, -1], train_data$Diabetes_binary, k = 5)
knn_predictions <- as.factor(knn_model)
knn_accuracy <- sum(knn_predictions == test_data$Diabetes_binary) / length(test_data$Diabetes_binary)
print(knn_accuracy)
```
71% accuracy becomes 99% accuracy by selecting 9 best features!


## ================================ Question 5 =================================

Given the application's requirement for fast prediction, it is important to consider the computational efficiency of the models. Parametric models, such as logistic regression, can quickly predict diabetes by performing a simple mathematical operation once the model is trained. These models estimate the relationship between the features (the same questions asked to the user) and the target variable using predetermined parameters.

On the other hand, non-parametric models like KNN require a more computationally intensive process for prediction. They involve calculating the distance between the new point and all other points in the training data. This process can be time-consuming, especially when dealing with large datasets. But the final result of this method in this task is better than others.

Another model worth considering is the decision tree. It is a parametric model that partitions the feature space into regions based on the provided features. Decision trees can be easily interpreted and understood. However, when selecting a model, it is crucial to consider not only its computational efficiency but also its accuracy and precision in classifying the data.

To evaluate the performance of the model, various metrics such as confusion matrix, F1-score, ROC curve, etc., can be utilized. It is essential to avoid overfitting or underfitting the model. Overfitting occurs when the model memorizes the training data, while underfitting happens when the model fails to capture the patterns in the data. Techniques like cross-validation, regularization, and pruning can be employed to address these issues and improve the model's generalization.

By considering these factors – computational efficiency, accuracy, precision, and avoidance of overfitting/underfitting – we can choose the most suitable model for predicting diabetes based on the given set of features obtained from the user.


